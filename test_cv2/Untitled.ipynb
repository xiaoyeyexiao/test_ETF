{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd51a5ae-50d8-469f-8400-5a22d2552852",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m colors \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m21\u001b[39m)])[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m palette\n\u001b[1;32m     32\u001b[0m colors \u001b[38;5;241m=\u001b[39m (colors \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m output_predictions_rgb \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyte\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m output_predictions_rgb\u001b[38;5;241m.\u001b[39mputpalette(colors)\n\u001b[1;32m     35\u001b[0m output_predictions_rgb\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_image_z.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tor13/lib/python3.8/site-packages/PIL/Image.py:2155\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2152\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreducing_gap must be 1.0 or greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 2155\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "\n",
    "# 加载预训练的 DeepLabv3 模型\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 图像预处理\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# 加载图像\n",
    "image_path = \"z.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)['out'][0]\n",
    "output_predictions = output.argmax(0)\n",
    "\n",
    "# 将预测结果保存为图像\n",
    "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
    "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
    "colors = (colors % 255).numpy().astype(\"uint8\")\n",
    "output_predictions_rgb = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
    "output_predictions_rgb.putpalette(colors)\n",
    "output_predictions_rgb.save(\"output_image_z.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b8b95f-285f-4759-bda7-e71b305ecd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def align_images(image1, image2):\n",
    "    # 初始化 SIFT 特征检测器\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    # 在两张图像上检测特征点和计算特征描述符\n",
    "    keypoints1, descriptors1 = sift.detectAndCompute(image1, None)\n",
    "    keypoints2, descriptors2 = sift.detectAndCompute(image2, None)\n",
    "\n",
    "    # 使用 FLANN 匹配器进行特征点匹配\n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    matches = flann.knnMatch(descriptors1, descriptors2, k=2)\n",
    "\n",
    "    # 选择最佳匹配\n",
    "    good_matches = []\n",
    "    for m, n in matches:\n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            good_matches.append(m)\n",
    "\n",
    "    # 收集匹配的特征点坐标\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # 计算变换矩阵\n",
    "    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    # 对第一张图像进行变换\n",
    "    aligned_image1 = cv2.warpPerspective(image1, M, (image2.shape[1], image2.shape[0]))\n",
    "\n",
    "    return aligned_image1\n",
    "\n",
    "# 读取两张图像\n",
    "image1 = cv2.imread(\"image1.jpg\", cv2.IMREAD_COLOR)\n",
    "image2 = cv2.imread(\"image2.jpg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "# 将图像转换为灰度图像\n",
    "gray_image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray_image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 对图像进行放缩以使其对齐\n",
    "aligned_image1 = align_images(gray_image1, gray_image2)\n",
    "\n",
    "# 显示对齐后的图像\n",
    "cv2.imshow(\"Aligned Image 1\", aligned_image1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4cc6eb75-3c56-49eb-a07b-78e68f6844ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f1b0596-e0e7-4556-9f37-d414efe856f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 加载预训练的 DeepLabv3 模型\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 图像预处理\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# 加载图像\n",
    "image_path = \"tt2.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# 获取原始图像尺寸\n",
    "original_image = Image.open(image_path)\n",
    "original_width, original_height = original_image.size\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)['out'][0]\n",
    "output_predictions = output.argmax(0)\n",
    "\n",
    "# 将预测结果调整为与原始图像相同的大小\n",
    "output_predictions_resized = transforms.Resize((original_height, original_width))(output_predictions.unsqueeze(0))\n",
    "output_predictions_resized = output_predictions_resized.squeeze(0)\n",
    "\n",
    "# 定义阈值，将人像区域与其他区域分隔开来\n",
    "threshold = 0.5\n",
    "face_mask = (output_predictions_resized == 15)  # 在 COCO 数据集上，15 是人的类别标签\n",
    "face_mask = face_mask.float() * 255  # 将布尔值转换为 0 或 255\n",
    "\n",
    "# 将二值图像转换为 OpenCV 格式\n",
    "face_mask_cv = face_mask.byte().cpu().numpy()\n",
    "\n",
    "# 寻找轮廓\n",
    "contours, _ = cv2.findContours(face_mask_cv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# 创建一个与原图大小相同的黑色图像\n",
    "black_image = np.zeros((original_height, original_width, 3), dtype=np.uint8)\n",
    "\n",
    "# 在黑色图像上绘制轮廓\n",
    "cv2.drawContours(black_image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# 将结果保存为图像\n",
    "output_image_path = \"output_contour_on_black_image_tt2.jpg\"\n",
    "cv2.imwrite(output_image_path, black_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6540aadf-0577-488f-b264-2807b9111433",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /io/opencv/modules/calib3d/src/fundam.cpp:385: error: (-28:Unknown error code -28) The input arrays should have at least 4 corresponding point sets to calculate Homography in function 'findHomography'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m image2 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image2_path)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 对图像进行对齐\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m aligned_image1 \u001b[38;5;241m=\u001b[39m \u001b[43malign_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# 保存对齐后的图像\u001b[39;00m\n\u001b[1;32m     53\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maligned_image1.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[58], line 35\u001b[0m, in \u001b[0;36malign_images\u001b[0;34m(image1, image2)\u001b[0m\n\u001b[1;32m     33\u001b[0m src_pts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(centers1)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     34\u001b[0m dst_pts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(best_matches)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m M, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindHomography\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_pts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRANSAC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# 对其中一个图像进行透视变换\u001b[39;00m\n\u001b[1;32m     38\u001b[0m aligned_image1 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpPerspective(image1, M, (image2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], image2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /io/opencv/modules/calib3d/src/fundam.cpp:385: error: (-28:Unknown error code -28) The input arrays should have at least 4 corresponding point sets to calculate Homography in function 'findHomography'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def extract_green_contours(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    lower_green = np.array([40, 40, 40])\n",
    "    upper_green = np.array([70, 255, 255])\n",
    "    mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    return contours\n",
    "\n",
    "def align_images(image1, image2):\n",
    "    contours1 = extract_green_contours(image1)\n",
    "    contours2 = extract_green_contours(image2)\n",
    "    \n",
    "    # 计算轮廓的中心点\n",
    "    centers1 = [np.mean(contour, axis=0) for contour in contours1]\n",
    "    centers2 = [np.mean(contour, axis=0) for contour in contours2]\n",
    "    \n",
    "    # 寻找最佳匹配\n",
    "    best_matches = []\n",
    "    for center1 in centers1:\n",
    "        min_distance = float('inf')\n",
    "        best_match = None\n",
    "        for center2 in centers2:\n",
    "            distance = np.linalg.norm(center1 - center2)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_match = center2\n",
    "        best_matches.append(best_match)\n",
    "    \n",
    "    # 计算透视变换矩阵\n",
    "    src_pts = np.float32(centers1).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32(best_matches).reshape(-1, 1, 2)\n",
    "    M, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    \n",
    "    # 对其中一个图像进行透视变换\n",
    "    aligned_image1 = cv2.warpPerspective(image1, M, (image2.shape[1], image2.shape[0]))\n",
    "    \n",
    "    return aligned_image1\n",
    "\n",
    "# 读取两张黑底图像\n",
    "image1_path = \"output_contour_on_black_image_tt2.jpg\"\n",
    "image2_path = \"output_contour_on_black_image_tt1.jpg\"\n",
    "\n",
    "image1 = cv2.imread(image1_path)\n",
    "image2 = cv2.imread(image2_path)\n",
    "\n",
    "# 对图像进行对齐\n",
    "aligned_image1 = align_images(image1, image2)\n",
    "\n",
    "# 保存对齐后的图像\n",
    "output_path = \"aligned_image1.png\"\n",
    "cv2.imwrite(output_path, aligned_image1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03750e1b-a188-4e86-839a-82daf2422dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 加载预训练的 DeepLabv3 模型\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 图像预处理\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# 加载图像\n",
    "image_path = \"z.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# 获取原始图像尺寸\n",
    "original_image = Image.open(image_path)\n",
    "original_width, original_height = original_image.size\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)['out'][0]\n",
    "output_predictions = output.argmax(0)\n",
    "\n",
    "# 将预测结果调整为与原始图像相同的大小\n",
    "output_predictions_resized = transforms.Resize((original_height, original_width))(output_predictions.unsqueeze(0))\n",
    "output_predictions_resized = output_predictions_resized.squeeze(0)\n",
    "\n",
    "# 定义阈值，将人像区域与其他区域分隔开来\n",
    "threshold = 0.5\n",
    "face_mask = (output_predictions_resized == 15)  # 在 COCO 数据集上，15 是人的类别标签\n",
    "face_mask = face_mask.float() * 255  # 将布尔值转换为 0 或 255\n",
    "\n",
    "# 将二值图像转换为 OpenCV 格式\n",
    "face_mask_cv = face_mask.byte().cpu().numpy()\n",
    "\n",
    "# 寻找轮廓\n",
    "contours, _ = cv2.findContours(face_mask_cv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# 将轮廓绘制在原始图像上\n",
    "output_image = cv2.cvtColor(np.array(original_image), cv2.COLOR_RGB2BGR)\n",
    "cv2.drawContours(output_image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# 将结果保存为图像\n",
    "output_image_path = \"output_contour_on_image_z.jpg\"\n",
    "cv2.imwrite(output_image_path, output_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19356024-500a-475e-9fc6-f74ee8c50bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 加载预训练的 DeepLabv3 模型\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 图像预处理\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return preprocess(image).unsqueeze(0)\n",
    "\n",
    "# 加载图像\n",
    "image_path = \"z.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# 获取原始图像尺寸\n",
    "original_image = Image.open(image_path)\n",
    "original_width, original_height = original_image.size\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)['out'][0]\n",
    "output_predictions = output.argmax(0)\n",
    "\n",
    "# 将预测结果调整为与原始图像相同的大小\n",
    "output_predictions_resized = transforms.Resize((original_height, original_width))(output_predictions.unsqueeze(0))\n",
    "output_predictions_resized = output_predictions_resized.squeeze(0)\n",
    "\n",
    "# 定义阈值，将人像区域与其他区域分隔开来\n",
    "threshold = 0.5\n",
    "face_mask = (output_predictions_resized == 15)  # 在 COCO 数据集上，15 是人的类别标签\n",
    "face_mask = face_mask.float() * 255  # 将布尔值转换为 0 或 255\n",
    "\n",
    "# 将二值图像转换为 OpenCV 格式\n",
    "face_mask_cv = face_mask.byte().cpu().numpy()\n",
    "\n",
    "# 寻找轮廓\n",
    "contours, _ = cv2.findContours(face_mask_cv, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# 创建一个与原图大小相同的黑色图像\n",
    "black_image = np.zeros((original_height, original_width, 3), dtype=np.uint8)\n",
    "\n",
    "# 在黑色图像上绘制轮廓\n",
    "cv2.drawContours(black_image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# 将结果保存为图像\n",
    "output_image_path = \"output_contour_on_black_image.jpg\"\n",
    "cv2.imwrite(output_image_path, black_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "519fc605-6207-4086-b47f-e49da3517a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mengqingyi/anaconda3/envs/tor13/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /home/mengqingyi/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b61d2689d742209dfd617e621cee68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/170M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# 加载预训练的 Mask R-CNN 模型\n",
    "model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# 图像预处理\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    return preprocess(image)\n",
    "\n",
    "# 加载图像\n",
    "image_path = \"tt1.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# 推理\n",
    "with torch.no_grad():\n",
    "    prediction = model([input_image])\n",
    "\n",
    "# 获取预测结果中的人脸掩码\n",
    "masks = prediction[0]['masks']\n",
    "masks = masks.detach().cpu().numpy()\n",
    "\n",
    "# 将人脸掩码应用到原始图像上\n",
    "original_image = cv2.imread(image_path)\n",
    "for i in range(masks.shape[0]):\n",
    "    mask = masks[i, 0]\n",
    "    mask = cv2.resize(mask, (original_image.shape[1], original_image.shape[0]))\n",
    "    original_image[mask > 0.5] = [0, 255, 0]  # 将人脸部分标记为绿色\n",
    "\n",
    "# 将结果保存为图像\n",
    "output_image_path = \"output_image_maskrnn.png\"\n",
    "cv2.imwrite(output_image_path, original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf6fc4ff-93ea-4fdd-aa6a-eec434dd6b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangles[[(1229, 527) (1601, 899)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# 加载 Dlib 的人脸检测器和关键点检测器\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# 加载图像\n",
    "image_path = \"ttbb.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 人脸检测\n",
    "faces = detector(gray)\n",
    "print(faces)\n",
    "# 遍历每张检测到的人脸\n",
    "for face in faces:\n",
    "    # 关键点检测\n",
    "    landmarks = predictor(gray, face)\n",
    "    \n",
    "    # 提取每个关键点的坐标\n",
    "    points = []\n",
    "    for n in range(68):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        points.append((x, y))\n",
    "        \n",
    "    # 在图像上绘制关键点\n",
    "    for point in points:\n",
    "        cv2.circle(image, point, 1, (0, 255, 0), -1)\n",
    "        \n",
    "# 将结果保存为图像\n",
    "output_image_path = \"output_image_with_landmarks.jpg\"\n",
    "cv2.imwrite(output_image_path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e85d56-650e-4e7f-a190-4b8769ba7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 读取人像分割后的图像，蓝色区域为人的上半身和头部\n",
    "segmented_image = cv2.imread('output_image_tt1.png')\n",
    "\n",
    "# 将图像转换为HSV颜色空间\n",
    "hsv_image = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# 定义蓝色区域的HSV阈值范围\n",
    "lower_blue = np.array([100, 50, 50])\n",
    "upper_blue = np.array([140, 255, 255])\n",
    "\n",
    "# 根据阈值范围进行颜色分割\n",
    "mask = cv2.inRange(hsv_image, lower_blue, upper_blue)\n",
    "\n",
    "# 找到轮廓\n",
    "contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# 框出头部\n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    cv2.rectangle(segmented_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# 显示结果图像\n",
    "cv2.imshow('Head Segmentation', segmented_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729b8624-ee80-4e94-a1f7-9e63d268a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 读取人像分割后的图像，蓝色区域为人的上半身和头部\n",
    "segmented_image = cv2.imread('output_image_tt1.png')\n",
    "\n",
    "# 将图像转换为灰度图像\n",
    "gray_image = cv2.cvtColor(segmented_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 使用阈值分割找到蓝色区域\n",
    "_, binary_image = cv2.threshold(gray_image, 100, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# 找到蓝色区域的外接矩形\n",
    "contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "x, y, w, h = cv2.boundingRect(contours[0])\n",
    "\n",
    "# 在原图上用矩形框出头部部分\n",
    "cv2.rectangle(segmented_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# 显示结果图像\n",
    "cv2.imshow('Head Detection', segmented_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62084b00-2055-43fd-a0b5-8be8fdb6f66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# 加载图像并转换为灰度图\n",
    "image_path = \"output_contour_on_black_image_tt2.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 使用阈值化进行分割\n",
    "_, thresholded_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# 寻找图像的轮廓\n",
    "contours, _ = cv2.findContours(thresholded_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# 寻找每个轮廓的凸包\n",
    "for contour in contours:\n",
    "    hull = cv2.convexHull(contour)\n",
    "    \n",
    "    # 在图像上绘制凸包的点\n",
    "    for point in hull:\n",
    "        x, y = point[0]\n",
    "        cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "# 显示标记后的图像\n",
    "cv2.imshow(\"Image with Convex Hull Points\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac000d7d-d9e0-45a0-b11a-e6bb3d30523b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# 加载dlib的面部关键点检测器\n",
    "predictor_path = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "# 加载图像\n",
    "image_path = \"tt2.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 检测人脸\n",
    "faces = detector(gray)\n",
    "\n",
    "# 遍历每张脸并标出面部关键点\n",
    "for face in faces:\n",
    "    landmarks = predictor(gray, face)\n",
    "    for n in range(0, 68):\n",
    "        x = landmarks.part(n).x\n",
    "        y = landmarks.part(n).y\n",
    "        # 放大关键点半径\n",
    "        cv2.circle(image, (x, y), 5, (0, 255, 0), -1)\n",
    "\n",
    "# 显示标出关键点的图像\n",
    "output_image_path = \"tt1_show_landmarks.jpg\"\n",
    "cv2.imwrite(output_image_path, image)\n",
    "\n",
    "# 保存标出关键点的图像\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d5231dd-250c-42f7-b870-78003ad1f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpened probabilities: [0.01362594 0.01362594 0.95525207 0.01749605]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 给定的列表\n",
    "logits = np.array([0.05, 0.05, 0.9, 0.1])\n",
    "# 温度参数\n",
    "tau = 0.2\n",
    "\n",
    "# 计算sharpen后的概率值\n",
    "sharpened_logits = np.exp(logits / tau)\n",
    "sharpened_probabilities = sharpened_logits / np.sum(sharpened_logits)\n",
    "\n",
    "print(\"Sharpened probabilities:\", sharpened_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf480db-ae1c-477d-b746-8498004bb364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
